{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "from poutyne.framework import Model, ModelCheckpoint, CSVLogger\n",
    "from poutyne import torch_to_numpy\n",
    "from poutyne.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "\n",
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "train_split_percent = 0.8\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "n_epoch = 5\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset and creating DataLoaders.\n",
    "\n",
    "train_dataset = MNIST('./mnist/', train=True, download=True, transform=transforms.ToTensor())\n",
    "valid_dataset = MNIST('./mnist/', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST('./mnist/', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "num_data = len(train_dataset)\n",
    "indices = list(range(num_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = math.floor(train_split_percent * num_data)\n",
    "\n",
    "train_indices = indices[:split]\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "\n",
    "valid_indices = indices[split:]\n",
    "valid_dataset = Subset(valid_dataset, valid_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fully_connected_network():\n",
    "    return nn.Sequential(\n",
    "        Flatten(),\n",
    "        nn.Linear(28*28, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, num_classes)\n",
    "    )\n",
    "\n",
    "def create_convolutional_network():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout(0.25),\n",
    "        Flatten(),\n",
    "        nn.Linear(64*14*14 , 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, num_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name, pytorch_module):\n",
    "    # One nice feature of Poutyne is callbacks.\n",
    "    callbacks = [\n",
    "        # Save the latest weights to be able to continue the optimization at the end for more epochs.\n",
    "        ModelCheckpoint(name + '_last_epoch.ckpt', temporary_filename='last_epoch.ckpt.tmp'),\n",
    "\n",
    "        # Save the weights in a new file when the current model is better than all previous models.\n",
    "        ModelCheckpoint(name + '_best_epoch_{epoch}.ckpt', monitor='val_acc', mode='max', save_best_only=True, restore_best=True, verbose=False, temporary_filename='best_epoch.ckpt.tmp'),\n",
    "\n",
    "        # Save the losses and accuracies for each epoch in a TSV.\n",
    "        CSVLogger(name + '_log.tsv', separator='\\t'),\n",
    "    ]\n",
    "    \n",
    "    # Finally, we start the training and output its final test \n",
    "    # loss and accuracy.\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = optim.SGD(pytorch_module.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Poutyne Model\n",
    "    model = Model(pytorch_module, optimizer, loss_function, metrics=['accuracy'])\n",
    "\n",
    "    # Send model on GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Train\n",
    "    model.fit_generator(train_loader, valid_loader, epochs=n_epoch, callbacks=callbacks)\n",
    "\n",
    "    # Test\n",
    "    test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "    print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5 9.23s Step 1500/1500: loss: 2.038025, acc: 37.295833, val_loss: 1.054108, val_acc: 68.633333\n",
      "Epoch 2/5 9.30s Step 1500/1500: loss: 0.613594, acc: 81.795833, val_loss: 0.438431, val_acc: 87.200000\n",
      "Epoch 3/5 10.32s Step 1500/1500: loss: 0.390026, acc: 88.685417, val_loss: 0.351080, val_acc: 89.925000\n",
      "Epoch 4/5 9.55s Step 1500/1500: loss: 0.322791, acc: 90.743750, val_loss: 0.305179, val_acc: 91.133333\n",
      "Epoch 5/5 9.81s Step 1500/1500: loss: 0.277492, acc: 92.050000, val_loss: 0.265470, val_acc: 92.533333\n",
      "Test:\n",
      "\tLoss: 0.25376319630146027\n",
      "\tAccuracy: 92.93\n"
     ]
    }
   ],
   "source": [
    "# Initialize my network\n",
    "fc_net = create_fully_connected_network()\n",
    "print(fc_net)\n",
    "\n",
    "# Start training\n",
    "train('fc', fc_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Dropout(p=0.25)\n",
      "  (6): Flatten()\n",
      "  (7): Linear(in_features=12544, out_features=128, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Dropout(p=0.5)\n",
      "  (10): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5 11.26s Step 1500/1500: loss: 0.617333, acc: 81.035417, val_loss: 0.266608, val_acc: 91.850000\n",
      "Epoch 2/5 11.45s Step 1500/1500: loss: 0.320636, acc: 90.189583, val_loss: 0.179926, val_acc: 94.291667\n",
      "Epoch 3/5 11.40s Step 1500/1500: loss: 0.268208, acc: 91.954167, val_loss: 0.159907, val_acc: 95.075000\n",
      "Epoch 4/5 11.36s Step 1500/1500: loss: 0.241208, acc: 92.604167, val_loss: 0.139435, val_acc: 95.816667\n",
      "Epoch 5/5 11.33s Step 1500/1500: loss: 0.220420, acc: 93.366667, val_loss: 0.129128, val_acc: 96.175000\n",
      "Test:\n",
      "\tLoss: 0.11721408634185791\n",
      "\tAccuracy: 96.45\n"
     ]
    }
   ],
   "source": [
    "# Initialize my network\n",
    "conv_net = create_convolutional_network()\n",
    "print(conv_net)\n",
    "\n",
    "# Start training\n",
    "train('conv', conv_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
